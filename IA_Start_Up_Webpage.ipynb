{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6c4545",
   "metadata": {},
   "source": [
    "# Création de l'Application Web\n",
    "\n",
    "L'idée est de produire une page web avec des représentations graphiques des données textes (via Streamlit).\n",
    "\n",
    "    Pour lancer l'application Streamlit:\n",
    "**streamlit hello**\n",
    "    \n",
    "    Pour lancer la page Web Streamlit\n",
    "**streamlit run C:\\Users\\blanc\\OpenClassrooms\\IA_Project6_Openclassrooms_IAstart-up\\IA_Start_Up_Webpage.py**\n",
    "\n",
    "**streamlit run ~\\IA_Start_Up_Webpage.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "017465d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "\n",
    "#Librairie de Tokenisation\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.tag.util import str2tuple\n",
    "\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "\n",
    "\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.sklearn\n",
    "\n",
    "#from gensim import corpora\n",
    "#import pyLDAvis.gensim\n",
    "#import gensim\n",
    "#from gensim.models import CoherenceModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07cc234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras import Model, preprocessing, regularizers\n",
    "from keras.applications import VGG16, EfficientNetB0\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "#from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image as p_img\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import cluster, metrics, preprocessing, manifold, decomposition\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics.cluster import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad07f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 09:40:37.671 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\blanc\\anaconda3\\envs\\IA_Projet6_Images_Webpage\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.title('IA Project 6 - WebPage to manage Text & Image data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699669b9",
   "metadata": {},
   "source": [
    "## Chargement des modèles\n",
    "\n",
    "Comme nous l'avons fait lors de l'analyse des données et des modèles, nous utilisons la **libraire Joblib** pour ré-utiliser nos modèls à partir de nouvelles données acquises depuis l'API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ef90c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chargement du modèle d'encodage tfidf entrainé précédemment? #tfidf = TfidfVectorizer(min_df=0.005,max_df=0.8)\n",
    "tfidf = load('C:/Users/blanc/OpenClassrooms/IA_Project6_Openclassrooms_IAstart-up/Models/tfidfvectorizer.joblib')\n",
    "\n",
    "# Chargement  du modèle LDA   #lda_tfidf.fit(values_tfidf)\n",
    "lda_tfidf_api = load('C:/Users/blanc/OpenClassrooms/IA_Project6_Openclassrooms_IAstart-up/Models/lda_tfidf.joblib')\n",
    "\n",
    "#Chargement du modèle d'encodage tfidf entrainé précédemment? #tfidf = TfidfVectorizer(min_df=0.005,max_df=0.8)\n",
    "#tfidf_gensim = load('C:/Users/blanc/OpenClassrooms/IA_Project6_Openclassrooms_IAstart-up/Models/tfidf_gensim.joblib')\n",
    "\n",
    "# Chargement  du modèle LDA  GENSIM\n",
    "#lda_gensim_api = load('C:/Users/blanc/OpenClassrooms/IA_Project6_Openclassrooms_IAstart-up/Models/lda_gensim.joblib'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17373a50",
   "metadata": {},
   "source": [
    "## Creation d'un pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2bf344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_nlp_pipeline(data: pd.DataFrame, max_score_filter=2, rating_column='rating', text_column='text', tag_type_to_eliminate=['NN','RB','RBR','IN','VB','VBN','VBG','VBZ','MD','CD','PRP','PRP$']):\n",
    "    #filter on unstasfied comments (rating under max_score_filter variable)\n",
    "    data = data.loc[(data[rating_column] <= max_score_filter), :].copy()\n",
    "    \n",
    "    # Change Text column to lower case\n",
    "    data[text_column] = data.loc[:,text_column].str.lower()\n",
    "    \n",
    "    # Replace punct by blank\n",
    "    data[text_column] = data.loc[:,text_column].str.replace(r'[^\\w\\s]+', '', regex=True)\n",
    "    #data[text_column] = data[text_column].str.replace(r'[^\\w\\s]+', '', regex=True)\n",
    "    \n",
    "    # Remove numbers digits from text column\n",
    "    data['text_only'] = data.loc[:,text_column].str.replace(r'[\\d]+', '', regex=True) \n",
    "    #data['text_only'] = data[text_column].str.replace('\\d+', '', regex=True)\n",
    "    \n",
    "    # Words splitting\n",
    "    data = tokenize_text(data, 'text_only')\n",
    "    #Remove all unecessary tagged words (Adverbs, Verbs, ...)\n",
    "    data = remove_undesired_wordtag(data, 'tokenized_text', tag_type_to_eliminate)\n",
    "    \n",
    "    # Words cleaning\n",
    "    data = stopwords_text(data, 'removed_tag_text')\n",
    "    data = lemmatize_text(data, 'stopwords_text')\n",
    "    \n",
    "    #Création du bag of words\n",
    "    data['thesaurus'] = data.loc[:,'lemmatized_text'].apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    #Extraction des features avec Tf-Idf Vectorizer : min_df=0.01,max_df=0.8 (précedemment entrainé, et chargé depuis Joblib précedemment)\n",
    "    values = tfidf.transform(data['thesaurus'])\n",
    "    #values = tfidf.fit_transform(data['thesaurus'])\n",
    "    #print(\"Created %d X %d TF-IDF-normalized document-term matrix\" % (values.shape[0], values.shape[1]) )\n",
    "    \n",
    "    #From the thesaurus text, split each word and create new dataframe counting word presence frequency\n",
    "    word_frequency = data.thesaurus.str.split(expand=True).stack().value_counts()\n",
    "    word_frequency = pd.DataFrame({'word':word_frequency.index, 'word_count':word_frequency.values})\n",
    "    #For each word, determne its TAG type\n",
    "    word_frequency['word_tag'] = word_frequency['word'].apply(lambda x: nltk.pos_tag([x])[0][1])\n",
    "    \n",
    "    return(data, values, word_frequency)\n",
    "\n",
    "# Init the Stopword language\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#Function to remove useless words \n",
    "def stopwords_text(df, column):\n",
    "    #df['stopwords_text'] = df[column].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    df['stopwords_text'] = df.loc[:,column].apply(lambda x: [w for w in x if not w in stop and w.isalpha()])\n",
    "    return (df)\n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "#Function to lemmatize text (Generic representation of words)\n",
    "def lemmatize_text(df, column):\n",
    "    #df['lemmatized_text'] = df.apply(lambda row: lemmatizer.lemmatize(row[column]), axis=1)\n",
    "    df['lemmatized_text'] = df[column].apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n",
    "    #df['lemmatized_text'] = df.loc[:,column].apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n",
    "    return (df)\n",
    "\n",
    "# Init the Tokenizer\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#Function to tokenize text (split in array)\n",
    "def tokenize_text(df, column):\n",
    "    df['tokenized_text'] = df.apply(lambda row: tokenizer.tokenize(row[column]), axis=1)\n",
    "    #df['tokenized_text'] = df.apply(lambda row: tokenizer.tokenize(row[column]))\n",
    "    return (df)\n",
    "\n",
    "def remove_undesired_wordtag(df, column, tag_type_to_eliminate):\n",
    "    #df['removed_tag_text'] = df.apply(lambda row: pos_tag(row[column]), axis=1)\n",
    "    df['removed_tag_text'] = df.apply(lambda row: ([s[0] for s in pos_tag(row[column]) if s[1] not in tag_type_to_eliminate]), axis=1)\n",
    "    return df\n",
    "\n",
    "def load_data(DATA_URL, nrows):\n",
    "    data_loaded = pd.read_csv(DATA_URL, nrows=nrows)\n",
    "    lowercase = lambda x: str(x).lower()\n",
    "    data_loaded.rename(lowercase, axis='columns', inplace=True)\n",
    "    return (data_loaded)\n",
    "\n",
    "def custom_nlp_pipeline2(text_review, tag_type_to_eliminate=['NN','RB','RBR','IN','VB','VBN','VBG','VBZ','MD','CD','PRP','PRP$']):\n",
    "    \n",
    "    # Change Text column to lower case\n",
    "    text_review_processed = text_review.lower()\n",
    "    \n",
    "    # Replace punct by blank\n",
    "    text_review_processed = re.sub(r'[^\\w\\s]','',text_review_processed)\n",
    "    #text_review_processed = text_review_processed.replace(r'[^\\w\\s]+', '', regex=True)\n",
    "    \n",
    "    # Remove numbers digits from text column\n",
    "    #text_review_processed = re.sub(r'[^\\d]','',text_review_processed)\n",
    "    #text_review_processed = text_review_processed.replace(r'[\\d]+', '', regex=True) \n",
    "    \n",
    "    # Words splitting\n",
    "    text_review_processed = word_tokenize(text_review_processed)\n",
    "    \n",
    "    #text_review_processed = [s for s in text_review_processed if s[1] not in tag_type_to_eliminate]\n",
    "    text_review_processed = [s[0] for s in pos_tag(text_review_processed) if s[1] not in tag_type_to_eliminate]\n",
    "    \n",
    "    # Words cleaning\n",
    "    text_review_processed = [word for word in text_review_processed if word not in stopwords.words('english')]\n",
    "\n",
    "    #text_review_processed = lemmatizer.lemmatize(text_review_processed)\n",
    "    text_review_processed = ' '.join([lemmatizer.lemmatize(w) for w in text_review_processed])\n",
    "     #Création du bag of words\n",
    "    #text_review_processed = text_review_processed.apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    text_review_processed = pd.Series(text_review_processed)\n",
    "    #d = {'thesaurus': text_review_processed}\n",
    "    #text_review_processed = pd.DataFrame(data=d)\n",
    "    \n",
    "    #Extraction des features avec Tf-Idf Vectorizer : min_df=0.01,max_df=0.8 (précedemment entrainé, et chargé depuis Joblib précedemment)\n",
    "    values = tfidf.transform(text_review_processed)\n",
    "    \n",
    "    return(text_review_processed, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958473a",
   "metadata": {},
   "source": [
    "## Chargement du fichier CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264d4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('C:/Users/blanc/Documents/ConnectERPrise_Logo.jpg')\n",
    "with st.sidebar:\n",
    "    st.image(image, caption='My Logo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70da5b4",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470481f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.header('Text Data Processing - From Existing CSV')\n",
    "st.subheader(f'Text Data Extract Process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5686ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose how many rows to import from CSV\n",
    "nrows = st.slider('How many reviews do you want to import from CSV?', 1, 10000, 5000)\n",
    "st.write(\"You selected \", nrows, 'number of reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0d5de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_file = st.file_uploader(\"Choose a CSV file corresponding to API GraphQL extract performed previously\")\n",
    "if uploaded_file is not None:\n",
    "        # Create a text element and let the reader know the data is loading.\n",
    "        data_load_state = st.text('Loading data...')\n",
    "        # Load n rows of data into the dataframe.\n",
    "        data_imported_csv = load_data(uploaded_file, nrows=nrows)\n",
    "        # Notify the reader that the data was successfully loaded.\n",
    "        data_load_state.text('Loading data...done!')\n",
    "        #st.write(dataframe)\n",
    "        st.write(f'Imported {nrows} rows from **CSV file {uploaded_file.name}**, are stored in Dataframe')\n",
    "        if st.checkbox('Show Imported dataframe'):\n",
    "            st.dataframe(data_imported_csv)\n",
    "        #st.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adf01513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#st.subheader(f'Imported {nrows} rows from CSV file, are stored in Dataframe')\n",
    "#st.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b98f705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.subheader(f'Text Data Transform Process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22aad38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define variables\n",
    "max_score_filter=2\n",
    "rating_column='rating'\n",
    "text_column='text'\n",
    "#tag_type_to_eliminate = ['NN','RB','RBR','IN','VB','VBN','VBG','VBZ','MD','CD','PRP','PRP$']\n",
    "tag_type_to_eliminate = ['RB','RBR','MD','CD']\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    #Rune the pipeline on df_api data\n",
    "    df_api_pipeline, values, word_frequency = custom_nlp_pipeline(pd.DataFrame(data_imported_csv), max_score_filter, rating_column, text_column, tag_type_to_eliminate)\n",
    "    st.write('Dataframe is filtered on negative evaluation, and associated reviews text are preprocessed : lowercase + punct + digits + tokenisation + stopwords + lemmatization + bag of words')\n",
    "    if st.checkbox('Show dataframe at Transformation Step 1 : NLP'):\n",
    "        st.dataframe(df_api_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef0794b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_topic_name(row):\n",
    "    if row['Topic'] == 0:\n",
    "        val = 'Nourriture, Aliments et Gouts'\n",
    "    elif row['Topic'] == 1:\n",
    "        val = 'Réservation'\n",
    "    elif row['Topic'] == 2:\n",
    "        val = 'Bar et boissons'\n",
    "    else:\n",
    "        val = 'Temps d attente et Service'\n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68ca1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 4\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    values_tfidf = pd.DataFrame(values.toarray())\n",
    "    topic_result_csv_df = pd.DataFrame(lda_tfidf_api.transform(values_tfidf)).idxmax(axis=1)\n",
    "    #values_tfidf.head()\n",
    "\n",
    "    df_api_results = pd.concat([df_api_pipeline.loc[:, ['rating', 'text', 'name']].reset_index(drop=True), pd.DataFrame(lda_tfidf_api.transform(values_tfidf)).reset_index(drop=True)], axis = 1)\n",
    "    df_api_results['Topic'] = topic_result_csv_df #pd.DataFrame(lda_tfidf_api.transform(values_tfidf)).idxmax(axis=1)\n",
    "    df_api_results['Topic_name'] = df_api_results.apply(simple_topic_name, axis=1)\n",
    "    if st.checkbox('Show dataframe at Transformation Step 2 : Topics Detection'):\n",
    "        #st.dataframe(df_api_results.style.highlight_max(axis=0))\n",
    "        st.dataframe(df_api_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baf7bf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.subheader(f'Text Data Visualization Process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "209eb70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.markdown('Here below, we show the imported **CSV file post-processing results**, and more specifically the _*number of reviews for each topic_*.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4467dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chart_data = df_api_results['Topic'] #df_api_results.loc[:, ['rating','topic']]\n",
    "if uploaded_file is not None:\n",
    "    chart_data = pd.DataFrame(df_api_results['Topic_name'].value_counts(dropna=True, sort=True))\n",
    "\n",
    "    st.bar_chart(chart_data)\n",
    "\n",
    "    with st.expander(\"See explanation\"):\n",
    "        st.write(f\"The chart above shows the {n_topics} Topics used to classify text reviews from Imported Data\")\n",
    "        st.write(\"The dataframe below lists Topics proportion\")\n",
    "        st.dataframe(chart_data)\n",
    "        \n",
    "        # Create and generate a word cloud image:\n",
    "        st.write(\"The picture below will show top50 most represented words in selected tag from negative evaluation\")\n",
    "        \n",
    "        options = st.multiselect('What tag do you want to visualise ?',\n",
    "                                 ['NN','JJ','VBN', 'VBG', 'VB', 'VBD', 'IN', 'NNS'], #['noun','adjective','verb past participle', 'verb present participle', 'verb, base form', 'verb, past tense', 'preposition or conjunction', 'noun, common, plural'],\n",
    "                                 ['NN','JJ','IN']\n",
    "                                 )\n",
    "        st.write('You selected:', options)\n",
    "        \n",
    "        #data_wordcloud = word_frequency.loc[word_frequency['word_tag'] == 'JJ', :].set_index('word').to_dict()['word_count']\n",
    "        data_wordcloud = word_frequency.loc[word_frequency['word_tag'].isin(options) == True, :].set_index('word').to_dict()['word_count']\n",
    "        wordcloud = WordCloud(max_words = 50, width=800, height=400).generate_from_frequencies(data_wordcloud)\n",
    "        # Display the generated image:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.axis(\"off\")\n",
    "        st.pyplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25aae4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da244169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.header('Data Processing - From New Manual Entry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "230eb025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.subheader(f'Data Extract & Transform & Visualization Process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f77ea4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value_goes_here = 'Write text review here'\n",
    "user_input = st.text_input(default_value_goes_here, default_value_goes_here)\n",
    "st.write('The text pre-processing and classification will now start based on your input', user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "351e0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_name(topic_result):\n",
    "    if topic_result == 0:\n",
    "        val = 'Nourriture, Aliments et Gouts'\n",
    "    elif topic_result == 1:\n",
    "        val = 'Réservation'\n",
    "    elif topic_result == 2:\n",
    "        val = 'Bar et boissons'\n",
    "    else:\n",
    "        val = 'Temps d attente et Service'\n",
    "    return val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93df1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if st.button('Press the button to launch the text processing phases'):\n",
    "    text_review_processed, values = custom_nlp_pipeline2(user_input, tag_type_to_eliminate)\n",
    "    st.write('the text after post-processing is : ', text_review_processed[0])\n",
    "    \n",
    "    #st.write('the post-processing text is associated to topic')\n",
    "    values_tfidf = pd.DataFrame(values.toarray())\n",
    "    topic_result = pd.DataFrame(lda_tfidf_api.transform(values_tfidf)).idxmax(axis=1)\n",
    "    st.write('The text processed is allocated to topic : ', topic_result[0], topic_name(topic_result[0]))\n",
    "else:\n",
    "    st.write('Text processing not started')\n",
    "\n",
    "\n",
    "#user_output = st.write('the post-processing text is', text_review_processed)\n",
    "#user_output\n",
    "#topic_assignation = st.write('the post-processing text is associated to topic', values)\n",
    "#topic_assignation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3028c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe6a7b78",
   "metadata": {},
   "source": [
    "### Image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.header('Images Processing - From New Manual Entry')\n",
    "st.subheader(f'Image Extract & Transform & Visualization Process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88bdea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load kmeans model in joblib files\n",
    "kmeans_VGG = load('C:/Users/blanc/OpenClassrooms/IA_Project6_Openclassrooms_IAstart-up/Models/kmeans_VGG.joblib')\n",
    "\n",
    "tsne_VGG = load('C:/Users/blanc/OpenClassrooms/IA_Project6_Openclassrooms_IAstart-up/Models/tsne_VGG.joblib')\n",
    "\n",
    "#load stdscaler used to transform data\n",
    "std_scale_VGG = load('C:/Users/blanc/OpenClassrooms/IA_Project6_Openclassrooms_IAstart-up/Models/std_scale_VGG.joblib')\n",
    "\n",
    "#load kmeans model after pca reduction in joblib files\n",
    "kmeans_VGG_reduit = load('C:/Users/blanc/OpenClassrooms/IA_Project6_Openclassrooms_IAstart-up/Models/kmeans_VGG_réduit.joblib')\n",
    "\n",
    "tsne_VGG_reduit = load('C:/Users/blanc/OpenClassrooms/IA_Project6_Openclassrooms_IAstart-up/Models/tsne_VGG_réduit.joblib')\n",
    "\n",
    "#load stdscaler after pca reduction used to transform data\n",
    "std_scale_VGG_réduit = load('C:/Users/blanc/OpenClassrooms/IA_Project6_Openclassrooms_IAstart-up/Models/std_scale_VGG_réduit.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a8ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_matrix(label_predicted):\n",
    "    if label_predicted == 0:\n",
    "        label_name = 'outside_inside_predicted'\n",
    "    elif label_predicted == 1:\n",
    "        label_name = 'inside_outside_predicted'\n",
    "    elif label_predicted == 2:\n",
    "        label_name = 'menu_predicted'\n",
    "    elif label_predicted == 3:\n",
    "        label_name = 'food_predicted'\n",
    "    elif label_predicted == 4:\n",
    "        label_name = 'drink_predicted'\n",
    "    return label_name\n",
    "\n",
    "\n",
    "\n",
    "def simple_label_name(row):\n",
    "    if row['label_predicted'] == 0:\n",
    "        label_predicted_name = 'outside'\n",
    "    elif row['label_predicted'] == 1:\n",
    "        label_predicted_name = 'inside'\n",
    "    elif row['label_predicted'] == 2:\n",
    "        label_predicted_name = 'menu'\n",
    "    elif row['label_predicted'] == 3:\n",
    "        label_predicted_name = 'food'\n",
    "    elif row['label_predicted'] == 4:\n",
    "        label_predicted_name = 'drink'\n",
    "    return label_predicted_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73884e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_image = ['drink', 'menu', 'inside', 'outside', 'food']\n",
    "\n",
    "default_path_goes_here = 'C:/Users/blanc/OpenClassrooms/IA_Project6_Openclassrooms_IAstart-up/dataset/photos/'\n",
    "folder_path = st.text_input('Write Image path folder here', default_path_goes_here)\n",
    "st.write('The image folder path is : ', folder_path, '   !!! WARNING: This path shall be accurate !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e965bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 22:50:11.940 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\blanc\\anaconda3\\envs\\IA_Projet6_Images_Webpage\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "uploaded_image_file = st.file_uploader(f\"Choose a Image file corresponding to one of following Category : {list_image}\", type=(\"jpg\",\"png\",\"jpeg\"))\n",
    "if uploaded_image_file is not None:\n",
    "        # Create a text element and let the reader know the data is loading.\n",
    "        image_load_state = st.text('Loading image...')\n",
    "        # Load image\n",
    "        image_imported = Image.open(uploaded_image_file)\n",
    "        image_path = folder_path + uploaded_image_file.name\n",
    "        st.write(f'Imported file path is {image_path}')\n",
    "        \n",
    "\n",
    "        \n",
    "        # Notify the reader that the data was successfully loaded.\n",
    "        image_load_state.text('Loading image...done!')\n",
    "        #st.write(dataframe)\n",
    "        st.write(f'Image has been successfully imported')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea5d7089",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_imported' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load an image from file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m img \u001b[38;5;241m=\u001b[39m load_img(\u001b[43mimage_imported\u001b[49m, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))  \u001b[38;5;66;03m# Charger l'image\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# convert the image pixels to a numpy array\u001b[39;00m\n\u001b[0;32m      5\u001b[0m img \u001b[38;5;241m=\u001b[39m img_to_array(img)  \u001b[38;5;66;03m# Convertir en tableau numpy\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_imported' is not defined"
     ]
    }
   ],
   "source": [
    "model = VGG16(weights=\"imagenet\",include_top=False,pooling=\"avg\")\n",
    "VGG_features = []\n",
    "\n",
    "\n",
    "if uploaded_image_file is not None:    \n",
    "    # load an image from file\n",
    "    img = load_img(image_path, target_size=(224, 224))  # Charger l'image\n",
    "\n",
    "    # convert the image pixels to a numpy array\n",
    "    img = img_to_array(img)  # Convertir en tableau numpy\n",
    "    \n",
    "    # reshape data for the model\n",
    "    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))  # Créer la collection d'images (un seul échantillon)\n",
    "    \n",
    "    # prepare the image for the VGG model\n",
    "    img = preprocess_input(img)  # Prétraiter l'image comme le veut VGG-16\n",
    "\n",
    "    # Extracting our features\n",
    "    features = model.predict(img)\n",
    "\n",
    "    VGG_features.append(features[0])\n",
    "        \n",
    "    #Standard scaler on img\n",
    "    df_VGG_features = pd.DataFrame(VGG_features)\n",
    "    VGG_features_scaled = std_scale_VGG.transform(df_VGG_features)\n",
    "    df_VGG_features_scaled = pd.DataFrame(VGG_features_scaled)\n",
    "\n",
    "    # Extracting our features\n",
    "    label_predicted = kmeans_VGG.predict(df_VGG_features_scaled)\n",
    "    st.write(f\"Image classification is  : {label_matrix(label_predicted)}\")\n",
    "    \n",
    "    if st.checkbox('Show Imported Image'):\n",
    "            st.image(image_imported, caption=label_matrix(label_predicted), width=None, use_column_width=None, clamp=False, channels=\"RGB\", output_format=\"auto\")\n",
    "        #st.write(data)\n",
    "    \n",
    "    if st.checkbox('Show image detected features using VGG model'):\n",
    "        #st.dataframe(df_api_results.style.highlight_max(axis=0))\n",
    "        st.write(\"df_VGG_features\")\n",
    "        st.dataframe(df_VGG_features)\n",
    "        st.write(\"df_VGG_features_scaled\")\n",
    "        st.dataframe(df_VGG_features_scaled)\n",
    "        st.write(\"VGG_features_scaled\")\n",
    "        st.write(VGG_features_scaled)\n",
    "\n",
    "\n",
    "#     if st.checkbox('Show T-SNE visualisation of predicted class'):\n",
    "#         df_VGG_features_scaled = st.file_uploader(\"Choose a df_VGG_features_scaled.csv file\")\n",
    "#         if df_VGG_features_scaled is not None:\n",
    "#             df_VGG_features_scaled_csv = pd.read_csv(df_VGG_features_scaled)\n",
    "#             st.write(f'Imported {df_VGG_features_scaled.name} in Dataframe')\n",
    "#             if st.checkbox('Show Imported df_VGG_features_scaled dataframe'):\n",
    "#                 st.dataframe(df_VGG_features_scaled_csv)\n",
    "                \n",
    "# #             df_tsne_VGG_csv['label_predicted_name'] = df_tsne_VGG_csv.apply(simple_label_name, axis=1)\n",
    "# #             df_tsne_VGG_imported_image = pd.DataFrame(X_tsne[:,0:2], columns=['tsne1', 'tsne2'])\n",
    "# #             df_tsne_VGG_imported_image[\"label\"] = np.nan\n",
    "# #             df_tsne_VGG_imported_image[\"label_predicted\"] = label_predicted\n",
    "# #             df_tsne_VGG_imported_image[\"label_predicted_name\"] = label_matrix(label_predicted)\n",
    "# #             df_tsne_VGG_to_display = pd.concat([df_tsne_VGG_csv, df_tsne_VGG_imported_image]) \n",
    "            \n",
    "#             df_VGG_features_scaled_concat = pd.concat([df_VGG_features_scaled_csv, df_VGG_features_scaled]) \n",
    "#             X_tsne = tsne_VGG.fit_transform(df_VGG_features_scaled_concat) #ou df_bow_VGG ou weighted_images_df['VGG_features'] ou VGG_features_scaled ou df_VGG_features_scaled\n",
    "#             df_tsne_VGG = pd.DataFrame(X_tsne[:,0:2], columns=['tsne1', 'tsne2'])\n",
    "#             df_tsne_VGG[\"label\"] = weighted_images_df['label']\n",
    "#             df_tsne_VGG.head()\n",
    "\n",
    "#             # Display the generated T-SNE\n",
    "#             #Create numpy array for the visualisation\n",
    "#             x = df_tsne_VGG_to_display[\"tsne1\"]\n",
    "#             y = df_tsne_VGG_to_display[\"tsne2\"] \n",
    "\n",
    "#             fig = plt.figure(figsize=(10, 10))\n",
    "#             plt.scatter(x, y)\n",
    "#             #plt.scatter(x, y, label=\"label_predicted_name\")\n",
    "\n",
    "#             st.balloons()\n",
    "#             st.pyplot(fig)\n",
    "                                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81681c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
